{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\"\"\"https://newsapi.org/v2/everything?\n",
    "language=en\n",
    "&from=2019-10-01\n",
    "&sources=cnbc\n",
    "&apiKey=749e4a9b92e046a386a4420df0de07ee\"\"\")\n",
    "\n",
    "requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = Template(\"\"\"\n",
    "https://newsapi.org/v2/everything?\\\n",
    "language=$language\\\n",
    "&from=$from\\\n",
    "&sources=$sources\\\n",
    "&apiKey=$apiKey\\\n",
    "&pageSize=$pageSize\\\n",
    "&page=$page\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = url_template.substitute({\"language\": \"en\", \"from\": \"2019-10-01\", \"sources\": \"cnbc\", \n",
    "                               \"apiKey\": \"749e4a9b92e046a386a4420df0de07ee\", \"pageSize\": 30, \"page\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_response[\"articles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in json_response:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles=[]\n",
    "for entity in json_response:\n",
    "    if(entity=='articles'):\n",
    "        for i in range(0,len(json_response[entity])):\n",
    "            news_articles.append(json_response['articles'][i]['content'])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium web driver.\n",
    "driver = webdriver.Chrome(\"C:/chromedriver_win32/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get and store CNBC news articles (text and JSON) in respective directories.\n",
    "\n",
    "def get_news_articles_cnbc (from_date=\"2019-10-01\", api_key=\"749e4a9b92e046a386a4420df0de07ee\", page_size=100, pages=3,\n",
    "                            start_page=1, json_data_dir=\"C:/\", text_data_dir=\"C:/\",\n",
    "                            chrome_driver=\"C:/Users/Administrator/chromedriver_win32/chromedriver\"):\n",
    "    # Create the URL template to query News API.\n",
    "    url_template = Template(\"\"\"\n",
    "    https://newsapi.org/v2/everything?\\\n",
    "    language=$language\\\n",
    "    &from=$from\\\n",
    "    &sources=$sources\\\n",
    "    &apiKey=$apiKey\\\n",
    "    &pageSize=$pageSize\\\n",
    "    &page=$page\\\n",
    "    \"\"\")\n",
    "    # Get the driver object.\n",
    "    driver = webdriver.Chrome(chrome_driver)\n",
    "    \n",
    "    # Iterate over a number of pages.\n",
    "    for page in range(start_page, pages):\n",
    "        url = url_template.substitute({\"language\": \"en\", \"from\": from_date, \"sources\": \"cnbc\", \n",
    "                                       \"apiKey\": api_key, \"pageSize\": page_size, \"page\": page})\n",
    "        response = requests.get(url)\n",
    "        print(response.json())\n",
    "        # Iterate over page size, as the number of articles retrieved is = page size.\n",
    "        for article in range(1, page_size):\n",
    "            # File name - Article + underscore + time.\n",
    "            news_article_fname = \"article_\" + time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            # Article - In text and JSON formats.\n",
    "            news_article_text = \"\"\n",
    "            news_article_json = \"\"\n",
    "            print(response.json().keys())\n",
    "            # Check if articles are fetched, if not - continue.\n",
    "            if (\"articles\" in response.json().keys()):\n",
    "                # Get article URL. We receive this by parsing the News API response.\n",
    "                news_url = response.json()[\"articles\"][article][\"url\"]\n",
    "                # Print the current status.\n",
    "                print(\"INFO: Fetching article - \" + str(article) + \" of page - \" + str(page) + \". URL - \" + news_url)\n",
    "                # Get page using the Chromium driver.\n",
    "                news_article = driver.get(news_url)\n",
    "                # From this page, get the inner HTML. This gets the entire page as one HTML Document.\n",
    "                news_article_html = driver.find_element_by_xpath(\"//*\").get_attribute(\"innerHTML\")\n",
    "                # Beautiful soup parsed.\n",
    "                news_article_bs = BeautifulSoup(news_article_html, \"lxml\")\n",
    "                # For CNBC, the structure is - one div with id = MainContent, and that consists of divs in class - group.\n",
    "                news_article_divs = news_article_bs.find(id=\"MainContent\").find_all(\"div\", class_=\"group\")\n",
    "                # Iterate over the divs to fetch and concatenate paragraphs.\n",
    "                for div in news_article_divs:\n",
    "                    # Get all paragraphs in that div.\n",
    "                    news_article_p = div.find_all(\"p\")\n",
    "                    for p in news_article_p:\n",
    "                        # Concatenate the text string with the new data from the paragraph tag of the news article.\n",
    "                        news_article_text = news_article_text + \"\\n\" + p.get_text()\n",
    "                # If the text is empty, don't dump to file.\n",
    "                if not (news_article_text.strip()==\"\"):\n",
    "                    # Write JSON, using JSON.DUMPS function.\n",
    "                    news_article_jfile = open(json_data_dir + \"/\" + news_article_fname + \".json\", \"w+\", encoding=\"utf-8\")\n",
    "                    news_article_jfile.write(json.dumps(response.json()[\"articles\"][article]))\n",
    "                    news_article_jfile.close()\n",
    "                    # Write Text.\n",
    "                    news_article_tfile = open(text_data_dir + \"/\" + news_article_fname + \".txt\", \"w+\", encoding=\"utf-8\")\n",
    "                    news_article_tfile.write(news_article_text.strip())\n",
    "                    news_article_tfile.close()\n",
    "    # Close Selenium Chrome driver.\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get and store CNBC news articles (text and JSON) in respective directories.\n",
    "\n",
    "def get_news_articles_bi (from_date=\"2019-10-01\", api_key=\"749e4a9b92e046a386a4420df0de07ee\", page_size=20, pages=10,\n",
    "                            start_page=1, json_data_dir=\"C:/\", text_data_dir=\"C:/\",\n",
    "                            chrome_driver=\"C:/Users/Administrator/chromedriver_win32/chromedriver\"):\n",
    "    # Create the URL template to query News API.\n",
    "    url_template = Template(\"\"\"\n",
    "    https://newsapi.org/v2/everything?\\\n",
    "    language=$language\\\n",
    "    &from=$from\\\n",
    "    &sources=$sources\\\n",
    "    &apiKey=$apiKey\\\n",
    "    &pageSize=$pageSize\\\n",
    "    &page=$page\\\n",
    "    \"\"\")\n",
    "    # Get the driver object.\n",
    "    driver = webdriver.Chrome(chrome_driver)\n",
    "    \n",
    "    # Iterate over a number of pages.\n",
    "    for page in range(start_page, pages):\n",
    "        url = url_template.substitute({\"language\": \"en\", \"from\": from_date, \"sources\": \"business-insider\", \n",
    "                                       \"apiKey\": api_key, \"pageSize\": page_size, \"page\": page})\n",
    "        response = requests.get(url)\n",
    "        # Iterate over page size, as the number of articles retrieved is = page size.\n",
    "        for article in range(1, page_size):\n",
    "            # File name - Article + underscore + time.\n",
    "            news_article_fname = \"article_\" + time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            # Article - In text and JSON formats.\n",
    "            news_article_text = \"\"\n",
    "            news_article_json = \"\"\n",
    "            # Check if articles are fetched, if not - continue.\n",
    "            if (\"articles\" in response.json().keys()):\n",
    "                # Get article URL. We receive this by parsing the News API response.\n",
    "                news_url = response.json()[\"articles\"][article][\"url\"]\n",
    "                # Print the current status.\n",
    "                print(\"INFO: Fetching article - \" + str(article) + \" of page - \" + str(page) + \". URL - \" + news_url)\n",
    "                # Get page using the Chromium driver.\n",
    "                news_article = driver.get(news_url)\n",
    "                # From this page, get the inner HTML. This gets the entire page as one HTML Document.\n",
    "                news_article_html = driver.find_element_by_xpath(\"//*\").get_attribute(\"innerHTML\")\n",
    "                # Beautiful soup parsed.\n",
    "                news_article_bs = BeautifulSoup(news_article_html, \"lxml\")\n",
    "                # For CNBC, the structure is - one div with id = MainContent, and that consists of divs in class - group.\n",
    "                news_article_divs = news_article_bs.find(id=\"the_bi_content\")\n",
    "                # If the divs is None, then continue with the loop without parsing.\n",
    "                if (news_article_divs is None):\n",
    "                    continue\n",
    "                news_article_divs = news_article_divs.find_all(\"div\")\n",
    "                # Iterate over the divs to fetch and concatenate paragraphs.\n",
    "                for div in news_article_divs:\n",
    "                    # Get all paragraphs in that div.\n",
    "                    news_article_p = div.find_all(\"p\")\n",
    "                    for p in news_article_p:\n",
    "                        # Concatenate the text string with the new data from the paragraph tag of the news article.\n",
    "                        news_article_text = news_article_text + \"\\n\" + p.get_text()\n",
    "                # If the text is empty, don't dump to file.\n",
    "                if not (news_article_text.strip()==\"\"):\n",
    "                    # Write JSON, using JSON.DUMPS function.\n",
    "                    news_article_jfile = open(json_data_dir + \"/\" + news_article_fname + \".json\", \"w+\", encoding=\"utf-8\")\n",
    "                    news_article_jfile.write(json.dumps(response.json()[\"articles\"][article]))\n",
    "                    news_article_jfile.close()\n",
    "                    # Write Text.\n",
    "                    news_article_tfile = open(text_data_dir + \"/\" + news_article_fname + \".txt\", \"w+\", encoding=\"utf-8\")\n",
    "                    news_article_tfile.write(news_article_text.strip())\n",
    "                    news_article_tfile.close()\n",
    "    # Close Selenium Chrome driver.\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_news_articles_cnbc(json_data_dir=\"C:/Users/Administrator/Documents/GitHub/newsreels/data/News/JSON\",\n",
    "                       text_data_dir=\"C:/Users/Administrator/Documents/GitHub/newsreels/data/News/Text\", start_page=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'feedback@businessinsider.com (Tyler Lauletta), Tyler Lauletta', 'title': \"Nick Bosa trolled Baker Mayfield by recreating the quarterback's most iconic college celebration\", 'description': 'ESPN Nick Bosa trolled Baker Mayfield while tearing through the Cleveland Browns offensive line on Monday night. After taking down Mayfield in the backfield, Bosa celebrated by pretending to wave a flag through the air and planting it in the ground. The move …', 'url': 'https://www.businessinsider.com/video-nick-bosa-trolls-baker-mayfield-flag-celebration-2019-10', 'urlToImage': 'https://image.businessinsider.com/5d9beeeee6a381404210c5f5?width=1200&format=jpeg', 'publishedAt': '2019-10-08T02:28:14Z', 'content': 'Nick Bosa and the San Francisco 49ers defense made life tough for Baker Mayfield on Monday night.\\r\\nThrough just one half of football, the 49ers forced three turnovers out of the Browns, with Bosa causing havoc for the Cleveland offensive line.\\r\\nAs the first h… [+1677 chars]'}\n",
      "INFO: Fetching article - 1 of page - 1. URL - https://www.businessinsider.com/video-nick-bosa-trolls-baker-mayfield-flag-celebration-2019-10\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Amir Ismael', 'title': 'How to lace up your sneakers 3 different ways for unique looks — a step-by-step guide', 'description': \"Sneakers are integral to casual style, but whether you realize it or not, how you lace your sneakers can hurt or enhance the overall look of your outfit. By paying close attention to how they're laced, you can add a unique flair to your personal style or simp…\", 'url': 'https://www.businessinsider.com/how-to-lace-up-sneakers', 'urlToImage': 'https://image.businessinsider.com/5d9270152e22af51073f3bb8?width=1200&format=jpeg', 'publishedAt': '2019-10-08T01:30:00Z', 'content': None}\n",
      "INFO: Fetching article - 2 of page - 1. URL - https://www.businessinsider.com/how-to-lace-up-sneakers\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Connie Chen and Remi Rosmarin', 'title': '15 online stores with free in-store pickup to save time and shipping fees — from Target, Nordstrom, and more', 'description': \"Even with the variety of speedy shipping options available these days, sometimes you want to go into a store and pick up a product yourself. Whether it's to check out the product in person or because you need it right away, buying a product in a store has a l…\", 'url': 'https://www.businessinsider.com/buy-online-pickup-in-store-same-day', 'urlToImage': 'https://image.businessinsider.com/5d5f08312e22af0c7c23f187?width=1200&format=jpeg', 'publishedAt': '2019-10-08T01:17:00Z', 'content': None}\n",
      "INFO: Fetching article - 3 of page - 1. URL - https://www.businessinsider.com/buy-online-pickup-in-store-same-day\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'David Choi', 'title': 'Trump administration considering pulling out of a vital treaty with Russia, lawmaker warns', 'description': \"The US is considering pulling out of a vital treaty with European allies and Russia, according to the chairman of the House Foreign Relations Committee. In a letter sent to National Security Adviser Robert O'Brien, Rep. Eliot Engel of New York on Monday, the …\", 'url': 'https://www.businessinsider.com/us-pulling-out-open-skies-treaty-report-2019-10', 'urlToImage': 'https://image.businessinsider.com/5d9bbca2754caa400760b68c?width=1200&format=jpeg', 'publishedAt': '2019-10-08T00:51:10Z', 'content': \"The US is considering pulling out of a vital treaty with European allies and Russia, according to the chairman of the House Foreign Relations Committee.\\r\\nIn a letter sent to National Security Adviser Robert O'Brien, Rep. Eliot Engel of New York on Monday said… [+4369 chars]\"}\n",
      "INFO: Fetching article - 4 of page - 1. URL - https://www.businessinsider.com/us-pulling-out-open-skies-treaty-report-2019-10\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Lauren Frias', 'title': 'A former Trump Organization vice president thinks Trump will resign before getting impeached, just like Nixon did', 'description': 'Barbara Res, a former Trump Organization vice president, told CNN\\'s Brian Stelter that she thinks President Trump will resign before the impeachment process is through. Res argued Trump has a track record of doing \"things to save face.\" The move mirrors that …', 'url': 'https://www.businessinsider.com/former-trump-organization-exec-thinks-trump-will-resign-before-impeachment-2019-10', 'urlToImage': 'https://image.businessinsider.com/5d9bbe809386bc419c36c8c7?width=1200&format=jpeg', 'publishedAt': '2019-10-07T23:21:55Z', 'content': 'A former Trump Organization executive said she thinks President Trump will resign before the impeachment inquiry, brought on by a whistleblower complaint, is complete.\\xa0\\r\\nThe move mirrors that of former President Richard Nixon, who resigned from office before … [+2099 chars]'}\n",
      "INFO: Fetching article - 5 of page - 1. URL - https://www.businessinsider.com/former-trump-organization-exec-thinks-trump-will-resign-before-impeachment-2019-10\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'James Pasley', 'title': 'These are the deepest and largest man-made holes in the world', 'description': \"Since the early 1960s, humans have attempted to drill down to the Earth's mantle. Russia holds the record for the deepest man-made hole in the world at more than 40,000 feet deep. That's 7.6 miles. No one has ever reached the Earth's mantle, although scientis…\", 'url': 'https://www.businessinsider.com/deepest-largest-holes-humans-have-dug-photos-2019-10', 'urlToImage': 'https://image.businessinsider.com/5d963eded6d2ef00267cb03c?width=1200&format=jpeg', 'publishedAt': '2019-10-07T22:12:16Z', 'content': \"America might have landed on the moon, but Russia drilled the deepest man-made hole on Earth.\\r\\nSince the early 1960s, scientists have attempted to drill down to the Earth's mantle.\\xa0\\r\\nIt took 20 years, but Russia drilled down 40,230 feet into the earth, before… [+496 chars]\"}\n",
      "INFO: Fetching article - 6 of page - 1. URL - https://www.businessinsider.com/deepest-largest-holes-humans-have-dug-photos-2019-10\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Morgan McFall-Johnsen', 'title': '20 new moons were just discovered orbiting Saturn, and you can help name them', 'description': \"Twenty new moons orbiting Saturn were discovered, bringing the planet's total up to 82 moons. Saturn now has more moons than any other planet in the solar system, including Jupiter, which has 79. Further study of the new moons could help determine how the pla…\", 'url': 'https://www.businessinsider.com/saturn-more-moons-than-jupiter-discovery-20-new-moons-2019-10', 'urlToImage': 'https://image.businessinsider.com/5d9b9b4272fd823c420cb4c4?width=1200&format=jpeg', 'publishedAt': '2019-10-07T21:56:42Z', 'content': \"Saturn has unseated Jupiter as the solar system's most moon-bearing planet, the Carnegie Institution for Science announced on Monday.\\xa0\\r\\nScientists discovered 20 previously unknown moons orbiting Saturn, which gives Saturn a grand total of 82 moons, flying pas… [+3426 chars]\"}\n",
      "INFO: Fetching article - 7 of page - 1. URL - https://www.businessinsider.com/saturn-more-moons-than-jupiter-discovery-20-new-moons-2019-10\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Devon Delfino', 'title': 'How to reduce the file size of a PDF document on a Mac computer in 4 simple steps', 'description': \"You can reduce a PDF's file size on a Mac computer in just a few steps. It's a good idea to reduce the file size of a PDF if you need to save storage space on your Mac or in the process of sending the file, but it may also reduce the quality of your PDF. Here…\", 'url': 'https://www.businessinsider.com/how-to-reduce-pdf-file-size-mac', 'urlToImage': 'https://image.businessinsider.com/5d9ba405707bdf3e4b0bedf4?width=1200&format=jpeg', 'publishedAt': '2019-10-07T21:51:00Z', 'content': 'When it comes to saving space on your Mac computer, there are many things you can do — empty the trash, delete app installers, get rid of duplicate files, and more.\\xa0\\r\\nYou can also reduce the file size of PDF documents in order to make them more compact, and t… [+1250 chars]'}\n",
      "INFO: Fetching article - 8 of page - 1. URL - https://www.businessinsider.com/how-to-reduce-pdf-file-size-mac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Kevin Webb', 'title': 'Apple is reportedly considering a subscription bundle for Apple Music and its streaming service Apple TV Plus, but the music industry is worried it will lose money on the deal (AAPL)', 'description': 'Apple is considering packaging Apple Music and the upcoming Apple TV Plus as a combined subscription, according to a new report from the Financial Times. Apple Music is currently available for $10 a month, and Apple TV Plus will launch on November 1 for $5 pe…', 'url': 'https://www.businessinsider.com/apple-is-considering-music-and-tv-plus-subscription-bundle-report-2019-10', 'urlToImage': 'https://image.businessinsider.com/5d9bac1d880f263d7b47c003?width=1200&format=jpeg', 'publishedAt': '2019-10-07T21:45:27Z', 'content': \"Apple TV Plus hasn't launched yet, but Apple has already considered packaging its on-demand video service with its streaming music service, Apple Music.\\r\\nA new report from Anna Nicolaou and Patrick McGee of the Financial Times says that Apple has entered into… [+1635 chars]\"}\n",
      "INFO: Fetching article - 9 of page - 1. URL - https://www.businessinsider.com/apple-is-considering-music-and-tv-plus-subscription-bundle-report-2019-10\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'James Pasley', 'title': '12 US states and 7 countries that have barred protesters from wearing masks', 'description': \"On Friday, October 4, Hong Kong leader Carrie Lam announced protesters were banned from wearing face masks, as she tried to deter violent protests that have been going on for four months. Hong Kong isn't the only place to have banned masks at demonstrations. …\", 'url': 'https://www.businessinsider.com/countries-states-where-protesters-cant-wear-masks-2019-10', 'urlToImage': 'https://image.businessinsider.com/5d9a1a7d4b650727fd02f995?width=1200&format=jpeg', 'publishedAt': '2019-10-07T21:35:40Z', 'content': 'On Friday, Hong Kong Leader Carrie Lam announced an immediate city-wide ban on protesters wearing face masks.\\r\\nShe said it had to be done, because most of the protesters vandalizing the city were doing it with their faces covered. The ability to hide their id… [+619 chars]'}\n",
      "INFO: Fetching article - 10 of page - 1. URL - https://www.businessinsider.com/countries-states-where-protesters-cant-wear-masks-2019-10\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Emma Witman', 'title': \"'What does HDR mean on an iPhone?': A guide to the iPhone's HDR camera mode, and how to control it manually\", 'description': 'On an iPhone, the acronym HDR stands for \"High dynamic range,\" and refers to the way an image is processed. With HDR on, every photo you take on your iPhone will be optimized with the best exposure settings. You can turn on HDR manually through the iPhone\\'s S…', 'url': 'https://www.businessinsider.com/what-does-hdr-mean-on-iphone', 'urlToImage': 'https://image.businessinsider.com/5d9ba1da32bf0f3be40a027c?width=1200&format=jpeg', 'publishedAt': '2019-10-07T21:35:00Z', 'content': \"One of the most remarkable aspects of the iPhone is its powerful camera. That's more true than ever with the recently announced iPhone 11 Pro, which features not just two, but three camera lenses.\\r\\nBut for those of us still living with our older iPhones, ther… [+1867 chars]\"}\n",
      "INFO: Fetching article - 11 of page - 1. URL - https://www.businessinsider.com/what-does-hdr-mean-on-iphone\n",
      "{'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Liz Knueven', 'title': 'A man who retired at 65 says his No. 1 rule for a simple retirement is clear: Be married', 'description': 'Bill Brown, who retired at age 65 and has been married to his wife for over 40 years, says that his number one rule for an easy retirement is to \"be married.\" In his own experience, he\\'s found it easier to live off of two incomes than it is to live off one, a…', 'url': 'https://www.businessinsider.com/being-married-makes-retirement-easier-says-retiree', 'urlToImage': 'https://image.businessinsider.com/5d9b94723e07743afd65aa42?width=1200&format=jpeg', 'publishedAt': '2019-10-07T21:30:50Z', 'content': 'Retiree Bill Brown and his wife have started off retirement comfortably, and have come to enjoy some time together (and apart) after years of juggling busy careers.\\xa0\\r\\nIn Brown\\'s opinion and experience, he tells Business Insider, \"the No. 1 rule for an early r… [+3037 chars]'}\n",
      "INFO: Fetching article - 12 of page - 1. URL - https://www.businessinsider.com/being-married-makes-retirement-easier-says-retiree\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f0f84ddede4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m get_news_articles_bi(json_data_dir=\"C:/Users/Administrator/Documents/GitHub/newsreels/data/News/JSON\",\n\u001b[1;32m----> 2\u001b[1;33m                        text_data_dir=\"C:/Users/Administrator/Documents/GitHub/newsreels/data/News/Text\")\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-b964493a911b>\u001b[0m in \u001b[0;36mget_news_articles_bi\u001b[1;34m(from_date, api_key, page_size, pages, start_page, json_data_dir, text_data_dir, chrome_driver)\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mnews_article_bs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_article_html\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;31m# For CNBC, the structure is - one div with id = MainContent, and that consists of divs in class - group.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mnews_article_divs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews_article_bs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"the_bi_content\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[1;31m# Iterate over the divs to fetch and concatenate paragraphs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mdiv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews_article_divs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "get_news_articles_bi(json_data_dir=\"C:/Users/Administrator/Documents/GitHub/newsreels/data/News/JSON\",\n",
    "                       text_data_dir=\"C:/Users/Administrator/Documents/GitHub/newsreels/data/News/Text\", start_page=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Format\n",
    "TRITTextResponse = TRITResponse.text\n",
    "#print(TRITTextResponse)\n",
    "\n",
    "#Json Format\n",
    "TRITJsonResponse = json.loads(TRITTextResponse)\n",
    "print(TRITJsonResponse)\n",
    "\n",
    "for entity in TRITJsonResponse:\n",
    "    for info in TRITJsonResponse[entity]:\n",
    "        topics={}\n",
    "        geography={}\n",
    "        organizations={}\n",
    "        persons={}\n",
    "        industries={}\n",
    "        currencies={}\n",
    "        market_index={}\n",
    "        \n",
    "        #Topics\n",
    "        if(info=='_typeGroup' and str(TRITJsonResponse[entity][info])=='topics'):\n",
    "            for companyinfo in (TRITJsonResponse[entity]):\n",
    "                if(companyinfo in ['_type','name','confidencelevel','score','relevance','permid']):\n",
    "                    topics[companyinfo]=str(TRITJsonResponse[entity][companyinfo])\n",
    "        if topics:\n",
    "            topics_list.append(topics)\n",
    "\n",
    "        \n",
    "        #Geographical Locations\n",
    "        if((info=='_type' and str(TRITJsonResponse[entity][info]) in ['Country','City','Continent','Region','ProvinceOrState'])):\n",
    "            for companyinfo in (TRITJsonResponse[entity]):\n",
    "                if(companyinfo in ['_type','name','confidencelevel','score','relevance','permid']):\n",
    "                    geography[companyinfo]=str(TRITJsonResponse[entity][companyinfo])\n",
    "                if(companyinfo=='resolutions'):\n",
    "                    if('permid' in TRITJsonResponse[entity][companyinfo][0]):  \n",
    "                        geography['permid']=str(TRITJsonResponse[entity][companyinfo][0]['permid'])\n",
    "                    if('id' in TRITJsonResponse[entity][companyinfo][0]):  \n",
    "                        geography['id']=str(TRITJsonResponse[entity][companyinfo][0]['id'])\n",
    "                    if('score' in TRITJsonResponse[entity][companyinfo][0]):  \n",
    "                        geography['score']=str(TRITJsonResponse[entity][companyinfo][0]['score'])\n",
    "                    if('ticker' in TRITJsonResponse[entity][companyinfo][0]):\n",
    "                        geography['ticker']=str(TRITJsonResponse[entity][companyinfo][0]['ticker'])\n",
    "        if geography:\n",
    "            geographical_list.append(geography)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
